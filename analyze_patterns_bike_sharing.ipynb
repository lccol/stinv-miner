{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "utility-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructField, StructType, BooleanType, StringType, DoubleType, IntegerType, ArrayType\n",
    "\n",
    "from stpm.visualization import plot_pattern\n",
    "from stpm.statistics import sum_delta_distrib, compute_mean_delta, count_delta_diff_from_zero, count_n_elements\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List, Union, Dict, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r stpm.zip stpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fixed-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('stpm.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accepting-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(s: str) -> List[List[str]]:\n",
    "    assert isinstance(s, str)\n",
    "    res = []\n",
    "    stack = []\n",
    "    accumulator = []\n",
    "    str_acc = ''\n",
    "    for c in s:\n",
    "        if c == '[':\n",
    "            stack.append(c)\n",
    "        elif c == ']':\n",
    "            stack.pop()\n",
    "            if len(stack) > 0:\n",
    "                if len(str_acc) > 0:\n",
    "                    accumulator.append(str_acc)\n",
    "                    str_acc = ''\n",
    "                res.append(accumulator)\n",
    "                accumulator = []\n",
    "        elif c == ',':\n",
    "            if len(str_acc) > 0:\n",
    "                accumulator.append(str_acc)\n",
    "                str_acc = ''\n",
    "        elif c == ' ':\n",
    "            continue\n",
    "        else:\n",
    "            str_acc += c\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "exotic-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = 'datasets/SFBayAreaBikeSharing/'\n",
    "mode = 'radius'\n",
    "subfolder = f'patterns_{mode}_100m_DEF/'\n",
    "files = [\n",
    "    f'global_{mode}.csv',\n",
    "    f'mv_{mode}.csv',\n",
    "    f'pa_{mode}.csv',\n",
    "    f'rc_{mode}.csv',\n",
    "    f'sf_{mode}.csv',\n",
    "    f'sj_{mode}.csv'\n",
    "]\n",
    "\n",
    "fullpaths = [basepath + subfolder + f for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "latter-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = spark.udf.register('seqparser', split, ArrayType(ArrayType(StringType())))\n",
    "filter_center = spark.udf.register('filter_center', lambda it: any('S0_T0' in x for x in it[0]), 'boolean')\n",
    "\n",
    "def load_files(paths: List[str], filter_data: bool=True) -> List[DataFrame]:\n",
    "    res = []\n",
    "    schema = StructType([\n",
    "        StructField('freq', IntegerType()),\n",
    "        StructField('relFreq', DoubleType()),\n",
    "        StructField('confidence', DoubleType()),\n",
    "        StructField('sequence', StringType())\n",
    "    ])\n",
    "    for f in paths:\n",
    "        df = spark.read.csv(f, header=None, inferSchema=False, schema=schema) \\\n",
    "                        .withColumn('sequence', splitter(F.col('sequence')))\n",
    "        if filter_data:\n",
    "            df = df.filter('filter_center(sequence)')\n",
    "        res.append(df)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "computational-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = load_files(fullpaths, filter_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "naval-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nrules_table(file_list: List[str], df_list: List[DataFrame]) -> pd.DataFrame:\n",
    "    res_dict = {'filename': [], 'count':[]}\n",
    "    for f, df in zip(file_list, df_list):\n",
    "        c = df.count()\n",
    "        print(f'{f} - Number of rules: {c}')\n",
    "        res_dict['filename'].append(f)\n",
    "        res_dict['count'].append(c)\n",
    "        \n",
    "    return pd.DataFrame(res_dict)\n",
    "\n",
    "def compute_statistics(file_list: List[str], df_list: List[DataFrame]) -> pd.DataFrame:\n",
    "    res_dict = defaultdict(list)\n",
    "    for f, df in zip(file_list, df_list):\n",
    "        df = df.cache()\n",
    "        mean_n_elements = df.rdd.map(lambda it: count_n_elements(it['sequence'])).mean()\n",
    "        mean_spatial_delta = compute_mean_delta(df, delta_type='spatial')\n",
    "        mean_temporal_delta = compute_mean_delta(df, delta_type='temporal')\n",
    "        npatterns_spat_delta_gt0 = df.rdd.filter(lambda it: count_delta_diff_from_zero(it['sequence'], delta_type='spatial') > 0).count()\n",
    "        npatterns_temp_delta_gt0 = df.rdd.filter(lambda it: count_delta_diff_from_zero(it['sequence'], delta_type='temporal') > 0).count()\n",
    "        npatterns_both_delta_gt0 = df.rdd.filter(lambda it: count_delta_diff_from_zero(it['sequence'], delta_type='both') > 0).count()\n",
    "        df = df.unpersist()\n",
    "        \n",
    "        res_dict['files'].append(f)\n",
    "        res_dict['tot_patterns'].append(df.count())\n",
    "        res_dict['mean_num_el'].append(mean_n_elements)\n",
    "        res_dict['mean_spat_delta'].append(mean_spatial_delta)\n",
    "        res_dict['mean_temp_delta'].append(mean_temporal_delta)\n",
    "        res_dict['patterns_spat_delta_gt0'].append(npatterns_spat_delta_gt0)\n",
    "        res_dict['patterns_temp_delta_gt0'].append(npatterns_temp_delta_gt0)\n",
    "        res_dict['patterns_both_delta_gt0'].append(npatterns_both_delta_gt0)\n",
    "        \n",
    "        print('#' * 50)\n",
    "        print(f)\n",
    "        print(f'total patterns: {df.count()}')\n",
    "        print(f'mean number of elements: {mean_n_elements}')\n",
    "        print(f'mean spatial delta: {mean_spatial_delta}')\n",
    "        print(f'mean temporal delta: {mean_temporal_delta}')\n",
    "        print(f'number of patterns with at least one element with spatial delta > 0: {npatterns_spat_delta_gt0} ({npatterns_spat_delta_gt0/df.count()})')\n",
    "        print(f'number of patterns with at least one element with temporal delta > 0: {npatterns_temp_delta_gt0} ({npatterns_temp_delta_gt0/df.count()})')\n",
    "        print(f'number of patterns with at least one element with spatial AND temporal delta > 0: {npatterns_both_delta_gt0} ({npatterns_both_delta_gt0/df.count()})')\n",
    "        \n",
    "    return pd.DataFrame(dict(res_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrules_table = generate_nrules_table(files, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_statistics(files, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_compression_df(relative_radius,\n",
    "                           absolute_radius,\n",
    "                           relative_incoming,\n",
    "                           absolute_incoming,\n",
    "                           file_list,\n",
    "                           mapper_dict=None) -> pd.DataFrame:\n",
    "    if mapper_dict is None:\n",
    "        mapper_dict = {x: x for x in file_list}\n",
    "    relative_radius_dfs = load_files(relative_radius, filter_data=True)\n",
    "    absolute_radius_dfs = load_files(absolute_radius, filter_data=False)\n",
    "    relative_incoming_dfs = load_files(relative_incoming, filter_data=True)\n",
    "    absolute_incoming_dfs = load_files(absolute_incoming, filter_data=False)\n",
    "\n",
    "    res_dict = {\n",
    "        'city': [],\n",
    "        'relative_radius': [],\n",
    "        'absolute_radius': [],\n",
    "        'radius_compression': [],\n",
    "        'radius_rules_saving': [],\n",
    "        'relative_incoming': [],\n",
    "        'absolute_incoming': [],\n",
    "        'incoming_compression': [],\n",
    "        'incoming_rules_saving': []\n",
    "    }\n",
    "    for f, rdf_radius, adf_radius, rdf_inc, adf_inc  in zip(files,\n",
    "                                                             relative_radius_dfs,\n",
    "                                                             absolute_radius_dfs,\n",
    "                                                             relative_incoming_dfs,\n",
    "                                                             absolute_incoming_dfs):\n",
    "        rcount_radius = rdf_radius.count()\n",
    "        acount_radius = adf_radius.count()\n",
    "        if acount_radius == 0:\n",
    "            print(f'found empty dataframe in absolute radius mode for file {f}')\n",
    "            saving_radius = -1\n",
    "        else:\n",
    "            saving_radius = 1 - rcount_radius / acount_radius\n",
    "        if rcount_radius == 0:\n",
    "            print(f'found empty dataframe in relative radius mode for file {f}')\n",
    "            compression_radius = -1\n",
    "        else:\n",
    "            compression_radius = acount_radius / rcount_radius\n",
    "        \n",
    "        \n",
    "        rcount_inc = rdf_inc.count()\n",
    "        acount_inc = adf_inc.count()\n",
    "        if acount_inc == 0:\n",
    "            print(f'found empty dataframe in absolute incoming mode for file {f}')\n",
    "            saving_inc = -1\n",
    "        else:\n",
    "            saving_inc = 1 - rcount_inc / acount_inc\n",
    "        if rcount_inc == 0:\n",
    "            print(f'found empty dataframe in relative incoming mode for file {f}')\n",
    "            compression_inc = -1\n",
    "        else:\n",
    "            compression_inc = acount_inc / rcount_inc\n",
    "        \n",
    "        res_dict['city'].append(mapper_dict[f])\n",
    "        res_dict['relative_radius'].append(rcount_radius)\n",
    "        res_dict['absolute_radius'].append(acount_radius)\n",
    "        res_dict['radius_compression'].append(compression_radius)\n",
    "        res_dict['radius_rules_saving'].append(saving_radius)\n",
    "        res_dict['relative_incoming'].append(rcount_inc)\n",
    "        res_dict['absolute_incoming'].append(acount_inc)\n",
    "        res_dict['incoming_compression'].append(compression_inc)\n",
    "        res_dict['incoming_rules_saving'].append(saving_inc)\n",
    "        \n",
    "    return pd.DataFrame(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### compute compression rate for the 100m case by comparing extracted projected patterns\n",
    "###### and extracted absolute patterns\n",
    "basepath = 'datasets/SFBayAreaBikeSharing/'\n",
    "\n",
    "relative_radius_path = basepath + 'patterns_radius_100m_DEF/'\n",
    "absolute_radius_path = basepath + 'patterns_radius_absolute_100m_DEF/'\n",
    "\n",
    "relative_incoming_path = basepath + 'patterns_incoming_100m_future_DEF/'\n",
    "absolute_incoming_path = basepath + 'patterns_incoming_absolute_100m_DEF/'\n",
    "\n",
    "files = [\n",
    "    'pa_{}.csv',\n",
    "    'mv_{}.csv',\n",
    "    'sj_{}.csv',\n",
    "    'rc_{}.csv'\n",
    "]\n",
    "\n",
    "radius_files = [x.replace('{}', 'radius') for x in files]\n",
    "incoming_files = [x.replace('{}', 'incoming') for x in files]\n",
    "\n",
    "relative_radius_fullpaths = [relative_radius_path + f for f in radius_files]\n",
    "absolute_radius_fullpaths = [absolute_radius_path + f for f in radius_files]\n",
    "relative_incoming_fullpaths = [relative_incoming_path + f for f in incoming_files]\n",
    "absolute_incoming_fullpaths = [absolute_incoming_path + f for f in incoming_files]\n",
    "\n",
    "mapper_dict = {\n",
    "    'pa_{}.csv': 'Palo Alto',\n",
    "    'mv_{}.csv': 'Mountain View',\n",
    "    'sj_{}.csv': 'San Jose',\n",
    "    'rc_{}.csv': 'Redwood City'\n",
    "}\n",
    "\n",
    "compression_df = compute_compression_df(relative_radius_fullpaths,\n",
    "                                        absolute_radius_fullpaths,\n",
    "                                        relative_incoming_fullpaths,\n",
    "                                        absolute_incoming_fullpaths,\n",
    "                                        files,\n",
    "                                        mapper_dict)\n",
    "compression_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
