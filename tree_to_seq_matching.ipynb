{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "frank-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark as ps\n",
    "\n",
    "from pyspark import RDD, SparkContext, Broadcast\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, IntegerType, BooleanType, DoubleType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from typing import Dict, Union, Optional, Tuple, List\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "victorian-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, event, parent=None, lchild=None, rchild=None, depth=-1, nid=-1):\n",
    "        self.event = event\n",
    "        self.parent = parent\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "        self.depth = depth\n",
    "        self.nid = nid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "downtown-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(s: str) -> List[List[str]]:\n",
    "    assert isinstance(s, str)\n",
    "    res = []\n",
    "    stack = []\n",
    "    accumulator = []\n",
    "    str_acc = ''\n",
    "    for c in s:\n",
    "        if c == '[':\n",
    "            stack.append(c)\n",
    "        elif c == ']':\n",
    "            stack.pop()\n",
    "            if len(stack) > 0:\n",
    "                if len(str_acc) > 0:\n",
    "                    accumulator.append(str_acc)\n",
    "                    str_acc = ''\n",
    "                res.append(accumulator)\n",
    "                accumulator = []\n",
    "        elif c == ',':\n",
    "            if len(str_acc) > 0:\n",
    "                accumulator.append(str_acc)\n",
    "                str_acc = ''\n",
    "        elif c == ' ':\n",
    "            continue\n",
    "        else:\n",
    "            str_acc += c\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "respected-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2tree(tree_str: str, translate_dict: Dict=None) -> Node:\n",
    "    elements = tree_str.split(' ')\n",
    "    prev = None\n",
    "    root = None\n",
    "    depth = 0\n",
    "    idx = 0\n",
    "    for el in elements:\n",
    "        if el == '-1':\n",
    "            depth -= 1\n",
    "            prev = prev.parent\n",
    "            continue\n",
    "        translated_el = el\n",
    "        if not translate_dict is None:\n",
    "            translated_el = translate_dict[el]\n",
    "        curr = Node(translated_el, parent=prev, depth=depth, nid=idx)\n",
    "        if root is None:\n",
    "            root = curr\n",
    "        if not prev is None:\n",
    "            if prev.lchild is None:\n",
    "                prev.lchild = curr\n",
    "            else:\n",
    "                prev.rchild = curr\n",
    "        depth += 1\n",
    "        prev = curr\n",
    "        idx += 1\n",
    "    return root\n",
    "\n",
    "def printtree(n, acc=None):\n",
    "    if n is None:\n",
    "        return\n",
    "    if acc is None:\n",
    "        acc = []\n",
    "    el = f'{n.event},d={n.depth} | '\n",
    "    acc.append(el)\n",
    "    print(el, end='')\n",
    "    printtree(n.lchild, acc)\n",
    "    printtree(n.rchild, acc)\n",
    "    acc.append('-1 | ')\n",
    "    print('-1 | ', end='')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polyphonic-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_constraint_rule(root: Node) -> List[Tuple[int, str, Union[str, int]]]:\n",
    "    prev = root\n",
    "    res = []\n",
    "    def _rec(n: Node):\n",
    "        if n is None:\n",
    "            return\n",
    "        res.append((n.nid, n.event, n.parent.nid if not n.parent is None else 'root'))\n",
    "        _rec(n.lchild)\n",
    "        _rec(n.rchild)\n",
    "        return\n",
    "    _rec(root)\n",
    "    return res\n",
    "\n",
    "def verify_constraint_rule(seq_row: Row,\n",
    "                           rules: List[Tuple[int, str, Union[str, int]]],\n",
    "                           check_exact: bool=False,\n",
    "                           offset: int=1) -> bool:\n",
    "    sequence = seq_row['sequence']\n",
    "    constraint_dict = {}\n",
    "    validity_seq = [[True for _ in range(len(sequence[y]))] for y in range(len(sequence))]\n",
    "    for rule in rules:\n",
    "        rid, event, constraint = rule\n",
    "        if constraint == 'root':\n",
    "            check = f'{event}_S0_T0' in sequence[0]\n",
    "            if not check:\n",
    "                return False\n",
    "            constraint_dict[rid] = 0\n",
    "            validity_seq[0][sequence[0].index(f'{event}_S0_T0')] = False\n",
    "        else:\n",
    "            min_idx = constraint_dict[constraint]\n",
    "            check = False\n",
    "            idx_value = -1\n",
    "            for idx in range(min_idx + offset, len(sequence)):\n",
    "                for index, (valid, x) in enumerate(zip(validity_seq[idx], sequence[idx])):\n",
    "                    if not valid:\n",
    "                        continue\n",
    "                    if x.startswith(event):\n",
    "                        validity_seq[idx][index] = False\n",
    "                        check = True\n",
    "                        break\n",
    "                if check:\n",
    "                    idx_value = idx\n",
    "                    break\n",
    "            if not check:\n",
    "                return False\n",
    "            constraint_dict[rid] = idx_value\n",
    "    if check_exact:\n",
    "        return all(all(not x for x in y) for y in validity_seq)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "resident-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_dict = {\n",
    "    '1': 'Snow',\n",
    "    '2': 'Rain',\n",
    "    '3': 'Construction',\n",
    "    '4': 'Congestion',\n",
    "    '5': 'Event',\n",
    "    '6': 'Fog',\n",
    "    '7': 'Lane-Blocked',\n",
    "    '8': 'Cold',\n",
    "    '9': 'Other',\n",
    "    '10': 'Storm',\n",
    "    '11': 'Broken-Vehicle',\n",
    "    '12': 'Incident-Weather',\n",
    "    '13': 'Precipitation-UNK',\n",
    "    '14': 'Hail-Other',\n",
    "    '15': 'Incident-Other',\n",
    "    '16': 'Flow-Incident',\n",
    "    '17': 'Accident',\n",
    "    '-1': '-1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pointed-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('freq', IntegerType()),\n",
    "    StructField('relFreq', DoubleType()),\n",
    "    StructField('confidence', DoubleType()),\n",
    "    StructField('sequence', StringType())\n",
    "])\n",
    "\n",
    "data_path = 'datasets/ShortLongTermTrafficIncidents/patterns_REVISED/patterns_boston_kdd_singlestep.csv'\n",
    "\n",
    "splitter = spark.udf.register('seqparser', split, ArrayType(ArrayType(StringType())))\n",
    "filter_center = spark.udf.register('filter_center', lambda it: any('S0_T0' in x for x in it[0]), 'boolean')\n",
    "\n",
    "df = spark.read.csv(data_path,\n",
    "                    header=False,\n",
    "                    schema=schema) \\\n",
    "                .withColumn('sequence', splitter(F.col('sequence'))) \\\n",
    "                .filter('filter_center(sequence)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "viral-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_non_matching_sequences(df: DataFrame, trees_constraints: Broadcast):\n",
    "#     def _match(row: Row) -> Row:\n",
    "#         if row['matched']:\n",
    "#             return Row(**row.asDict())\n",
    "#         check = False\n",
    "#         for tree_constr in trees_constraints.value:\n",
    "#             check = verify_constraint_rule(row, tree_constr, check_exact=True, offset=0)\n",
    "#             if check:\n",
    "#                 break\n",
    "#         if not check:\n",
    "#             return Row(**row.asDict())\n",
    "#         res = row.asDict()\n",
    "#         res['matched'] = True\n",
    "#         return Row(**res)\n",
    "#     df.printSchema()\n",
    "#     df2 = df.withColumn('matched', F.lit(False)) \\\n",
    "#             .rdd \\\n",
    "#             .map(_match) \\\n",
    "#             .toDF()\n",
    "#     df2.printSchema()\n",
    "    \n",
    "#     return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "reasonable-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "3560\n",
      "ShortTerm -> STInv matches:\n",
      "Total matches: 1546 over 3560 STInv sequences (0.43426966292134833)\n",
      "STInv -> ShortTerm matches:\n",
      "Total matches: 100 over 111 trees (0.9009009009009009)\n"
     ]
    }
   ],
   "source": [
    "tree_df = pd.read_csv('kdd_lstw_extraction/frequent_trees_City_MSF-10_MTL-25_BO_mycopy2.csv')\n",
    "trees = [str2tree(x, translation_dict) for x in tree_df['Pattern'].tolist()]\n",
    "weather_events = {'Rain', 'Fog', 'Cold', 'Snow', 'Storm', 'Hail-Other', 'Hail', 'Precipitation-UNK'}\n",
    "filtered_trees = [t for t in trees if not t.event in weather_events]\n",
    "filtered_rules = [generate_constraint_rule(t) for t in filtered_trees]\n",
    "\n",
    "print(len(filtered_trees))\n",
    "\n",
    "filtered_sequences = df.filter('freq >= 10') \\\n",
    "                .rdd \\\n",
    "                .filter(lambda it: not any(any(x.startswith(t) for t in weather_events for x in y) for y in it['sequence'])) \\\n",
    "                .collect()\n",
    "print(len(filtered_sequences))\n",
    "\n",
    "match_shortlong = [False for _ in range(len(filtered_trees))]\n",
    "match_stinv = [False for _ in range(len(filtered_sequences))]\n",
    "\n",
    "for idtree, tree_rule in enumerate(filtered_rules):\n",
    "    for idseq, seq in enumerate(filtered_sequences):\n",
    "        check = verify_constraint_rule(seq, tree_rule, check_exact=True, offset=0)\n",
    "        if check:\n",
    "            match_shortlong[idtree] = True\n",
    "            match_stinv[idseq] = True\n",
    "            \n",
    "print(f'ShortTerm -> STInv matches:')\n",
    "print(f'Total matches: {sum(match_stinv)} over {len(match_stinv)} STInv sequences ({sum(match_stinv) / len(match_stinv)})')\n",
    "print(f'STInv -> ShortTerm matches:')\n",
    "print(f'Total matches: {sum(match_shortlong)} over {len(match_shortlong)} trees ({sum(match_shortlong) / len(match_shortlong)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "apparent-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construction,d=0 | Congestion,d=1 | -1 | Congestion,d=1 | -1 | -1 | \n",
      "Construction,d=0 | Flow-Incident,d=1 | -1 | -1 | \n",
      "Construction,d=0 | Accident,d=1 | -1 | -1 | \n",
      "Congestion,d=0 | Congestion,d=1 | Congestion,d=2 | Congestion,d=3 | -1 | Congestion,d=3 | -1 | -1 | Congestion,d=2 | -1 | -1 | Congestion,d=1 | -1 | -1 | \n",
      "Congestion,d=0 | Congestion,d=1 | Congestion,d=2 | Congestion,d=3 | Congestion,d=4 | -1 | Congestion,d=4 | -1 | -1 | -1 | Congestion,d=2 | -1 | -1 | -1 | \n",
      "Congestion,d=0 | Congestion,d=1 | Congestion,d=2 | Congestion,d=3 | Congestion,d=4 | Congestion,d=5 | -1 | -1 | -1 | -1 | Congestion,d=2 | -1 | -1 | -1 | \n",
      "Event,d=0 | Congestion,d=1 | -1 | Congestion,d=1 | -1 | -1 | \n",
      "Event,d=0 | Congestion,d=1 | -1 | Event,d=1 | -1 | -1 | \n",
      "Event,d=0 | Event,d=1 | Congestion,d=2 | -1 | -1 | -1 | \n",
      "Event,d=0 | Event,d=1 | Congestion,d=2 | -1 | Event,d=2 | -1 | -1 | -1 | \n",
      "Event,d=0 | Event,d=1 | Event,d=2 | Event,d=3 | -1 | -1 | -1 | -1 | \n"
     ]
    }
   ],
   "source": [
    "df_dict = {'trees': []}\n",
    "for flag, tree in zip(match_shortlong, filtered_trees):\n",
    "    if not flag:\n",
    "        strtree = ''.join(printtree(tree))\n",
    "        print('')\n",
    "        df_dict['trees'].append(strtree)\n",
    "        \n",
    "pd.DataFrame(df_dict).to_csv('not_matching_trees.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "revised-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(match_stinv) == len(filtered_sequences)\n",
    "df_dict = {'sequence': []}\n",
    "for flag, seq in zip(match_stinv, filtered_sequences):\n",
    "    if not flag:\n",
    "        df_dict['sequence'].append(seq['sequence'])\n",
    "        \n",
    "pd.DataFrame(df_dict).to_csv('not_matching_sequences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-dialogue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
